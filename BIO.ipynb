{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BIO.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1UpwAcUpH2m",
        "outputId": "fae71024-2456-4309-e620-66098b4b659c"
      },
      "source": [
        "%reset"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt5MjvotI3oi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f9c61cd-6a2c-428f-8620-2a7f0b4b758a"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di_uIe8Cne95"
      },
      "source": [
        "import os\n",
        "from natsort import natsorted, ns\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import keras\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from keras import models\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Activation, BatchNormalization, Input, Dropout, Flatten, Concatenate\n",
        "from keras.models import Model, Sequential\n",
        "from keras import backend as K\n",
        "from keras.layers import Lambda\n",
        "from keras import optimizers\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve\n",
        "from tensorflow.keras import initializers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hx0Th2_EI9HN"
      },
      "source": [
        "PATH_ORG = \"/content/drive/MyDrive/Dataset/Signature_Set1/genuine\"\n",
        "PATH_FORG = \"/content/drive/MyDrive/Dataset/Signature_Set1/forged\""
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jffAa5nJ79y5"
      },
      "source": [
        "PATH_ORG = \"/content/drive/MyDrive/Dataset/Signature_Set2/full_org\"\n",
        "PATH_FORG = \"/content/drive/MyDrive/Dataset/Signature_Set2/full_forg\""
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ulj_S1Vn_gu"
      },
      "source": [
        "def get_list(path_o, path_fo):\n",
        "  org_path = os.listdir(path_o)\n",
        "  forg_path = os.listdir(path_fo)\n",
        "  org_imgs = [path for path in org_path if path.endswith(\".png\")]\n",
        "  forg_imgs = [path for path in forg_path if path.endswith(\".png\")]\n",
        "  org_imgs = natsorted(org_imgs, alg=ns.IGNORECASE)\n",
        "  forg_imgs = natsorted(forg_imgs, alg=ns.IGNORECASE)\n",
        "  return org_imgs, forg_imgs\n",
        " "
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "356YX550b_1N"
      },
      "source": [
        "org_imgs, forg_imgs = get_list(PATH_ORG, PATH_FORG)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9Hkq7j_hQD1"
      },
      "source": [
        "org_imgs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dM-Evue81Ra6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d258e1d5-00e7-4c84-d1b8-a93a86653c57"
      },
      "source": [
        "print(len(org_imgs))\n",
        "print(len(forg_imgs))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120\n",
            "120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLiyYKAYrs9K"
      },
      "source": [
        "def create_df(originals,forgeries, path_o, path_fo):\n",
        "  image_1_o, image_2_o, label_o = [], [], []\n",
        "  image_1_f, image_2_f, label_f = [], [], []\n",
        "\n",
        "  # for writer_nr in range(55): # for the first dataset\n",
        "  #   for sig_A_nr in range(24):\n",
        "  #     for sig_B_nr in range(24):\n",
        "  #       image_1_o.append(os.path.join(path_o, originals[writer_nr*24 + sig_A_nr]))\n",
        "  #       image_2_o.append(os.path.join(path_o, originals[writer_nr*24 + sig_B_nr]))\n",
        "  #       label_o.append(0)\n",
        "\n",
        "  for writer_nr in range(24): # for the second dataset\n",
        "    for sig_A_nr in range(5):\n",
        "      for sig_B_nr in range(5):\n",
        "        image_1_o.append(os.path.join(path_o, originals[writer_nr*5 + sig_A_nr]))\n",
        "        image_2_o.append(os.path.join(path_o, originals[writer_nr*5 + sig_B_nr]))\n",
        "        label_o.append(0)\n",
        "\n",
        "  # for writer_nr in range(55): # for the first dataset\n",
        "  #   for sig_A_nr in range(24):\n",
        "  #     for sig_B_nr in range(24):\n",
        "  #       image_1_f.append(os.path.join(path_o, originals[writer_nr*24 + sig_A_nr]))\n",
        "  #       image_2_f.append(os.path.join(path_fo, forgeries[writer_nr*24 + sig_B_nr]))\n",
        "  #       label_f.append(1)\n",
        "\n",
        "  for writer_nr in range(24): \n",
        "    for sig_A_nr in range(5):\n",
        "      for sig_B_nr in range(5):\n",
        "        image_1_f.append(os.path.join(path_o, originals[writer_nr*5 + sig_A_nr]))\n",
        "        image_2_f.append(os.path.join(path_fo, forgeries[writer_nr*5 + sig_B_nr]))\n",
        "        label_f.append(1)\n",
        "  \n",
        "  image_1 = image_1_o + image_1_f\n",
        "  image_2 = image_2_o + image_2_f\n",
        "  label = label_o + label_f\n",
        "\n",
        "  data = {\"image_1\": image_1, \"image_2\": image_2, \"label\": label}\n",
        "  df = pd.DataFrame(data, columns = [\"image_1\",\"image_2\",\"label\"])\n",
        "  #df=df.reindex(np.random.permutation(df.index))\n",
        "  return df"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ud-VdE3Gtl_i",
        "outputId": "6e5b9211-3b0d-465c-f53a-020a102326dc"
      },
      "source": [
        "df = create_df(org_imgs, forg_imgs, PATH_ORG, PATH_FORG)\n",
        "print(df.shape)\n",
        "print(df.iloc[73,0])"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1200, 3)\n",
            "/content/drive/MyDrive/Dataset/Signature_Set1/genuine/NFI-00305003.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK1OBJv-TlPu",
        "outputId": "815906fc-b148-4278-b063-5be98c6d5399"
      },
      "source": [
        "# df_test = df[0:3456] # 6912/63360=10,9%\n",
        "# df_test2 = df[31680:35136]\n",
        "# df_test=df_test.append(df_test2, ignore_index=True)\n",
        "# df_test.shape\n",
        "\n",
        "df_test = df[0:75] # 150/1200 = 12,5%\n",
        "df_test2 = df[600:675]\n",
        "df_test=df_test.append(df_test2, ignore_index=True)\n",
        "df_test.shape"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icr8ygxrXRhC",
        "outputId": "36be57c9-b757-450c-f02a-868cbb68822c"
      },
      "source": [
        "# df_val = df[3456:8640] # 10368/63360=16,36%\n",
        "# df_val2 = df[35136:40320]\n",
        "# df_val=df_val.append(df_val2, ignore_index=True)\n",
        "# df_val.shape\n",
        "\n",
        "df_val = df[75:175] # 200/1200 = 16,7%\n",
        "df_val2 = df[675:775]\n",
        "df_val=df_val.append(df_val2, ignore_index=True)\n",
        "df_val.shape"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kn10cmfMY3eb",
        "outputId": "477e05e6-3aad-49ba-e6f5-91fcc2b5b0cd"
      },
      "source": [
        "# df1 = df[8640:31680] # 46080/63360=72,72%\n",
        "# df2 = df[40320:]\n",
        "# df_train=df1.append(df2, ignore_index=True)\n",
        "# df_train.shape\n",
        "\n",
        "df1 = df[175:600] # 850/1200=70,8%\n",
        "df2 = df[775:]\n",
        "df_train=df1.append(df2, ignore_index=True)\n",
        "df_train.shape"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(850, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCSQQtzXurtz",
        "outputId": "9e5fecef-d23b-49ff-a5db-1e6e28413983"
      },
      "source": [
        "df_train = df_train.sample(frac = 1) \n",
        "df_test = df_test.sample(frac = 1) \n",
        "df_val = df_val.sample(frac = 1) \n",
        "print(df_test.head)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method NDFrame.head of                                                image_1  ... label\n",
            "81   /content/drive/MyDrive/Dataset/Signature_Set1/...  ...     1\n",
            "82   /content/drive/MyDrive/Dataset/Signature_Set1/...  ...     1\n",
            "32   /content/drive/MyDrive/Dataset/Signature_Set1/...  ...     0\n",
            "102  /content/drive/MyDrive/Dataset/Signature_Set1/...  ...     1\n",
            "23   /content/drive/MyDrive/Dataset/Signature_Set1/...  ...     0\n",
            "..                                                 ...  ...   ...\n",
            "127  /content/drive/MyDrive/Dataset/Signature_Set1/...  ...     1\n",
            "99   /content/drive/MyDrive/Dataset/Signature_Set1/...  ...     1\n",
            "56   /content/drive/MyDrive/Dataset/Signature_Set1/...  ...     0\n",
            "103  /content/drive/MyDrive/Dataset/Signature_Set1/...  ...     1\n",
            "19   /content/drive/MyDrive/Dataset/Signature_Set1/...  ...     0\n",
            "\n",
            "[150 rows x 3 columns]>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LI-V-_u8ak0",
        "outputId": "fcb7cdf1-27e2-4a03-9fd4-7398b8123343"
      },
      "source": [
        "nr= 603\n",
        "print(df.iloc[nr,0])\n",
        "print(df.iloc[nr,1])\n",
        "print(df.iloc[nr,2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Dataset/Signature_Set1/genuine/NFI-00101001.png\n",
            "/content/drive/MyDrive/Dataset/Signature_Set1/forged/NFI-00102019.png\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhiCTHC8JRAI"
      },
      "source": [
        "class DataGenerator(keras.utils.Sequence):\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, dim=(71,110), n_channels=3, shuffle=False):\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.df = df\n",
        "        self.labels = df[\"label\"]\n",
        "        self.n_channels = n_channels\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        " \n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        " \n",
        "    def __getitem__(self, index):\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        rows = [self.df.iloc[k] for k in indexes]\n",
        "        X, y = self.__data_generation(rows)\n",
        "        return X, y\n",
        " \n",
        "    def on_epoch_end(self):\n",
        "        self.indexes = np.arange(self.df.shape[0])\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        " \n",
        "    def __data_generation(self, rows):\n",
        "        x_1 = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "        x_2 = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "        y = np.empty((self.batch_size), dtype=float)\n",
        "        \n",
        "        for i in range(len(rows)):\n",
        "            image_1 = cv2.imread(rows[i][\"image_1\"])\n",
        "            image_1 = cv2.resize(image_1,(110,72),cv2.INTER_AREA)\n",
        "            image_1 = cv2.bitwise_not(image_1)\n",
        "            image_1=np.array(image_1)\n",
        "            image_2 = cv2.imread(rows[i][\"image_2\"])\n",
        "            image_2 = cv2.resize(image_2,(110,72), cv2.INTER_AREA)\n",
        "            image_2 = cv2.bitwise_not(image_2)\n",
        "            image_2=np.array(image_2)\n",
        "          \n",
        "            x_1[i,] = image_1/255\n",
        "            x_2[i,] = image_2/255\n",
        "            y[i] = rows[i][\"label\"]\n",
        " \n",
        "        return [x_1, x_2], y"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykVW1WVkKCrq"
      },
      "source": [
        "df_test.to_csv('out.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofXCtgp5rQKj"
      },
      "source": [
        "#https://www.kaggle.com/arpandhatt/siamese-neural-networks\n",
        "# We have 2 inputs, 1 for each picture\n",
        "left_input = Input((110,72,3))\n",
        "right_input = Input((110,72,3))\n",
        "\n",
        "# We will use 2 instances of 1 network for this task\n",
        "convnet = Sequential([\n",
        "    Conv2D(5,3, input_shape=(110,72,3)),\n",
        "    Activation('relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(5,3),\n",
        "    Dropout(0.5),\n",
        "    Activation('relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(7,2),\n",
        "    Dropout(0.5),\n",
        "    Activation('relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(7,2),\n",
        "    Dropout(0.5),\n",
        "    Activation('relu'),\n",
        "    Flatten(),\n",
        "    Dense(18),\n",
        "    Activation('sigmoid')\n",
        "])\n",
        "# Connect each 'leg' of the network to each input\n",
        "# Remember, they have the same weights\n",
        "encoded_l = convnet(left_input)\n",
        "encoded_r = convnet(right_input)\n",
        "\n",
        "# Getting the L1 Distance between the 2 encodings\n",
        "L1_layer = Lambda(lambda tensor:K.abs(tensor[0] - tensor[1]))\n",
        "\n",
        "# Add the distance function to the network\n",
        "L1_distance = L1_layer([encoded_l, encoded_r])\n",
        "\n",
        "prediction = Dense(1,activation='sigmoid')(L1_distance)\n",
        "siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
        "\n",
        "optimizer = Adam(0.001, decay=2.5e-4)\n",
        "#//TODO: get layerwise learning rates and momentum annealing scheme described in paperworking\n",
        "siamese_net.compile(loss=\"binary_crossentropy\",optimizer=optimizer,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQfvElvXn5yi",
        "outputId": "f2bc8288-7f6a-420a-b8bc-c1617d8b2bb2"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, 110, 72, 3)] 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_8 (InputLayer)            [(None, 110, 72, 3)] 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential_3 (Sequential)       (None, 18)           9054        input_7[0][0]                    \n",
            "                                                                 input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 18)           0           sequential_3[0][0]               \n",
            "                                                                 sequential_3[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 1)            19          lambda_3[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 9,073\n",
            "Trainable params: 9,073\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnViSlOj8jCJ"
      },
      "source": [
        " params={\n",
        "    'dim': (72,110),\n",
        "    'batch_size': 32,\n",
        "    'n_channels': 3,\n",
        "    'shuffle': False\n",
        "}"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhdQM1tldXls"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "checkpointer = ModelCheckpoint(filepath='model.{epoch:02d}.h5', verbose=1, save_best_only=False, save_weights_only=False, save_freq='epoch')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dv-RBCtz8-b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff189fca-829b-4511-8689-8d35dc3d3b10"
      },
      "source": [
        "steps=len(df_train)/32\n",
        "val_steps = len(df_val)/32\n",
        "\n",
        "train_datagen = DataGenerator(df_train,**params)\n",
        "validation_datagen = DataGenerator(df_val,**params)\n",
        "siamese_net.fit(train_datagen,validation_data=validation_datagen, epochs=10, steps_per_epoch=steps, validation_steps=val_steps, use_multiprocessing=True, workers=6, callbacks=[checkpointer])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "1440/1440 [==============================] - ETA: 0s - loss: 0.1886 - accuracy: 0.9152WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "\n",
            "Epoch 00001: saving model to model.01.h5\n",
            "1440/1440 [==============================] - 522s 362ms/step - loss: 0.1886 - accuracy: 0.9152 - val_loss: 0.1458 - val_accuracy: 0.9770\n",
            "Epoch 2/10\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "1440/1440 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 1.0000WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "\n",
            "Epoch 00002: saving model to model.02.h5\n",
            "1440/1440 [==============================] - 451s 313ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.1420 - val_accuracy: 0.9633\n",
            "Epoch 3/10\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "1439/1440 [============================>.] - ETA: 0s - loss: 0.0064 - accuracy: 1.0000WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "\n",
            "Epoch 00003: saving model to model.03.h5\n",
            "1440/1440 [==============================] - 457s 317ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.1173 - val_accuracy: 0.9769\n",
            "Epoch 4/10\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "1439/1440 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "\n",
            "Epoch 00004: saving model to model.04.h5\n",
            "1440/1440 [==============================] - 454s 315ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.1666 - val_accuracy: 0.9390\n",
            "Epoch 5/10\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "1439/1440 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "\n",
            "Epoch 00005: saving model to model.05.h5\n",
            "1440/1440 [==============================] - 444s 309ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.1815 - val_accuracy: 0.9354\n",
            "Epoch 6/10\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "1440/1440 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "\n",
            "Epoch 00006: saving model to model.06.h5\n",
            "1440/1440 [==============================] - 445s 309ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.1503 - val_accuracy: 0.9475\n",
            "Epoch 7/10\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "1440/1440 [==============================] - ETA: 0s - loss: 8.5598e-04 - accuracy: 1.0000WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "\n",
            "Epoch 00007: saving model to model.07.h5\n",
            "1440/1440 [==============================] - 444s 308ms/step - loss: 8.5598e-04 - accuracy: 1.0000 - val_loss: 0.2174 - val_accuracy: 0.9207\n",
            "Epoch 8/10\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "1439/1440 [============================>.] - ETA: 0s - loss: 5.5844e-04 - accuracy: 1.0000WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "\n",
            "Epoch 00008: saving model to model.08.h5\n",
            "1440/1440 [==============================] - 444s 308ms/step - loss: 5.5822e-04 - accuracy: 1.0000 - val_loss: 0.1722 - val_accuracy: 0.9385\n",
            "Epoch 9/10\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "1440/1440 [==============================] - ETA: 0s - loss: 3.5857e-04 - accuracy: 1.0000WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "\n",
            "Epoch 00009: saving model to model.09.h5\n",
            "1440/1440 [==============================] - 460s 320ms/step - loss: 3.5857e-04 - accuracy: 1.0000 - val_loss: 0.1307 - val_accuracy: 0.9556\n",
            "Epoch 10/10\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "1440/1440 [==============================] - ETA: 0s - loss: 2.4348e-04 - accuracy: 1.0000WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "\n",
            "Epoch 00010: saving model to model.10.h5\n",
            "1440/1440 [==============================] - 456s 316ms/step - loss: 2.4348e-04 - accuracy: 1.0000 - val_loss: 0.1928 - val_accuracy: 0.9350\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f327db6f438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-3GvFi-SFIf"
      },
      "source": [
        "from keras.models import load_model"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_dkqzwJ5ISw",
        "outputId": "072e2f64-8f6d-4be7-b695-1bf5bc5bd9c7"
      },
      "source": [
        "model = load_model('/content/model.01.h5')\n",
        "params={\n",
        "    'dim': (72,110),\n",
        "    'batch_size': 32,\n",
        "    'n_channels': 3,\n",
        "    'shuffle': False\n",
        "}\n",
        "checkpointer = ModelCheckpoint(filepath='model2.{epoch:02d}.h5', verbose=1, save_best_only=False, save_weights_only=False, save_freq='epoch')\n",
        "steps=len(df_train)/32\n",
        "val_steps = len(df_val)/32\n",
        "\n",
        "train_datagen = DataGenerator(df_train,**params)\n",
        "validation_datagen = DataGenerator(df_val,**params)\n",
        "model.fit(train_datagen,validation_data=validation_datagen, epochs=10, steps_per_epoch=steps, validation_steps=val_steps, use_multiprocessing=True, workers=6, callbacks=[checkpointer])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "23/26 [========================>.....] - ETA: 2s - loss: 0.7845 - accuracy: 0.5815WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "27/26 [==============================] - ETA: 0s - loss: 0.7392 - accuracy: 0.6100WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "\n",
            "Epoch 00001: saving model to model2.01.h5\n",
            "27/26 [==============================] - 30s 1s/step - loss: 0.7392 - accuracy: 0.6100 - val_loss: 0.5359 - val_accuracy: 0.7277\n",
            "Epoch 2/10\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "24/26 [==========================>...] - ETA: 1s - loss: 0.5302 - accuracy: 0.7266WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "27/26 [==============================] - ETA: 0s - loss: 0.5176 - accuracy: 0.7303WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "\n",
            "Epoch 00002: saving model to model2.02.h5\n",
            "27/26 [==============================] - 27s 998ms/step - loss: 0.5176 - accuracy: 0.7303 - val_loss: 0.5814 - val_accuracy: 0.7054\n",
            "Epoch 3/10\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "26/26 [============================>.] - ETA: 0s - loss: 0.3891 - accuracy: 0.8413WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "27/26 [==============================] - ETA: 0s - loss: 0.3919 - accuracy: 0.8391WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "\n",
            "Epoch 00003: saving model to model2.03.h5\n",
            "27/26 [==============================] - 25s 925ms/step - loss: 0.3919 - accuracy: 0.8391 - val_loss: 0.5869 - val_accuracy: 0.6920\n",
            "Epoch 4/10\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "24/26 [==========================>...] - ETA: 1s - loss: 0.3348 - accuracy: 0.8672WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "27/26 [==============================] - ETA: 0s - loss: 0.3469 - accuracy: 0.8565WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "\n",
            "Epoch 00004: saving model to model2.04.h5\n",
            "27/26 [==============================] - 27s 1s/step - loss: 0.3469 - accuracy: 0.8565 - val_loss: 0.5750 - val_accuracy: 0.7009\n",
            "Epoch 5/10\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "23/26 [========================>.....] - ETA: 1s - loss: 0.3198 - accuracy: 0.8845WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "27/26 [==============================] - ETA: 0s - loss: 0.3160 - accuracy: 0.8843WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "\n",
            "Epoch 00005: saving model to model2.05.h5\n",
            "27/26 [==============================] - 23s 868ms/step - loss: 0.3160 - accuracy: 0.8843 - val_loss: 0.5868 - val_accuracy: 0.6607\n",
            "Epoch 6/10\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "24/26 [==========================>...] - ETA: 1s - loss: 0.2897 - accuracy: 0.9076WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "27/26 [==============================] - ETA: 0s - loss: 0.2877 - accuracy: 0.9109WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "\n",
            "Epoch 00006: saving model to model2.06.h5\n",
            "27/26 [==============================] - 27s 1s/step - loss: 0.2877 - accuracy: 0.9109 - val_loss: 0.6042 - val_accuracy: 0.6429\n",
            "Epoch 7/10\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "26/26 [============================>.] - ETA: 0s - loss: 0.2859 - accuracy: 0.9087WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "27/26 [==============================] - ETA: 0s - loss: 0.2841 - accuracy: 0.9109WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "\n",
            "Epoch 00007: saving model to model2.07.h5\n",
            "27/26 [==============================] - 29s 1s/step - loss: 0.2841 - accuracy: 0.9109 - val_loss: 0.5886 - val_accuracy: 0.6741\n",
            "Epoch 8/10\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "26/26 [============================>.] - ETA: 0s - loss: 0.2739 - accuracy: 0.9231WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "27/26 [==============================] - ETA: 0s - loss: 0.2757 - accuracy: 0.9236WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "\n",
            "Epoch 00008: saving model to model2.08.h5\n",
            "27/26 [==============================] - 28s 1s/step - loss: 0.2757 - accuracy: 0.9236 - val_loss: 0.5755 - val_accuracy: 0.6786\n",
            "Epoch 9/10\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.2618 - accuracy: 0.9212WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "27/26 [==============================] - ETA: 0s - loss: 0.2615 - accuracy: 0.9225WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "\n",
            "Epoch 00009: saving model to model2.09.h5\n",
            "27/26 [==============================] - 27s 1s/step - loss: 0.2615 - accuracy: 0.9225 - val_loss: 0.5610 - val_accuracy: 0.6964\n",
            "Epoch 10/10\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.2308 - accuracy: 0.9538WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "27/26 [==============================] - ETA: 0s - loss: 0.2327 - accuracy: 0.9525WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "\n",
            "Epoch 00010: saving model to model2.10.h5\n",
            "27/26 [==============================] - 27s 1s/step - loss: 0.2327 - accuracy: 0.9525 - val_loss: 0.5487 - val_accuracy: 0.7009\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f92b82aea90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAt6bo5xAofQ"
      },
      "source": [
        "model = load_model('/content/model2.10.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTpKPE_lPNnj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "139bbcad-b0b3-4b0d-8c0e-6a195a35dd35"
      },
      "source": [
        "test_datagen = DataGenerator(df_test,**params) # first dataset\n",
        "loss, acc = model.evaluate(test_datagen,steps=3, verbose=1)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3/3 [==============================] - 8s 3s/step - loss: 0.1781 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yF18WeJfAerw",
        "outputId": "f050a3d0-7cce-4583-a56f-3aefa482d4c0"
      },
      "source": [
        "test_datagen = DataGenerator(df_test,**params) # second dataset\n",
        "loss, acc = model.evaluate(test_datagen,steps=3, verbose=1)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3/3 [==============================] - 1s 282ms/step - loss: 0.3435 - accuracy: 0.8229\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNgFUVMYttSY",
        "outputId": "293a15fd-65fb-4a6b-e643-94d9b60d0f28"
      },
      "source": [
        "im_1 = cv2.imread(\"/content/drive/MyDrive/Dataset/Signature_Set1/forged/NFI-00101014.png\")\n",
        "im_2 = cv2.imread(\"/content/drive/MyDrive/Dataset/Signature_Set1/genuine/NFI-00103001.png\")  \n",
        "im_1 = cv2.resize(im_1,(110,71))\n",
        "im_2 = cv2.resize(im_2,(110,71))\n",
        "im_1 = cv2.bitwise_not(im_1)\n",
        "im_2 = cv2.bitwise_not(im_2)\n",
        "im_1 = im_1/255\n",
        "im_2 = im_2/255\n",
        "im_1 = np.expand_dims(im_1,axis=0)\n",
        "im_2 = np.expand_dims(im_2,axis=0)\n",
        " \n",
        "y_pred = model.predict([im_1,im_2])\n",
        "if y_pred < 0.5:\n",
        "  wynik = \"oryginał\"\n",
        "else:\n",
        "  wynik = \"podrobiony\"\n",
        "print(y_pred, wynik)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.9945199]] podrobiony\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}